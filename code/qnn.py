import pennylane as qml
import torch.nn as nn
# import torch.nn.functional as F
import torch
import numpy as np
import os


# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
# Quantum Network: 
n_layers = 1  # 电路层数
n_qubits_1 = 4  # 8+9 16+17比特？
n_qubits_2 = 5
# qml.device 指定后端模拟器和量子比特数，这里default.qubit模拟纯态qubit，也可以用default.mixed模拟 lightning.qubit CPU lightning.gpu gpu
dev_1 = qml.device('lightning.qubit', wires=n_qubits_1, batch_obs = True)
dev_2 = qml.device('lightning.qubit', wires=n_qubits_2, batch_obs = True)


# 构建编码电路
def embedding_circuit(inputs, n_qubits):
    for qub in range(n_qubits):
        qml.Hadamard(wires=qub)
        qml.RY(inputs[qub], wires=qub)


# 构建卷积层
def conv_circuit_1(layer, n_qubits, weights):
    param_index = 0
    for i in range(n_qubits-1):
        qml.RZ(-np.pi / 2, wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[(i + 1) % n_qubits, i])
        qml.RZ(weights[layer, param_index], wires=i)
        qml.RY(weights[layer, param_index + 1], wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[i, (i + 1) % n_qubits])
        qml.RY(weights[layer, param_index + 2], wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[(i + 1) % n_qubits, i])
        qml.RZ(np.pi / 2, wires=i)

        param_index += 3

def conv_circuit_2(layer, n_qubits, weights):
    param_index = 0
    for i in range(n_qubits-1):
        qml.RZ(-np.pi / 2, wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[(i + 1) % n_qubits, i])
        qml.RZ(weights[layer, param_index], wires=i)
        qml.RY(weights[layer, param_index + 1], wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[i, (i + 1) % n_qubits])
        qml.RY(weights[layer, param_index + 2], wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[(i + 1) % n_qubits, i])
        qml.RZ(np.pi / 2, wires=i)
        param_index += 3 

def pool_circuit(layer, n_qubits, weights):
    param_index = 0
    for i in range(n_qubits-1):
        qml.RZ(-np.pi / 2, wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[(i + 1) % n_qubits, i])
        qml.RZ(weights[layer, param_index], wires=i)
        qml.RY(weights[layer, param_index + 1], wires=(i + 1) % n_qubits)
        qml.CNOT(wires=[i, (i + 1) % n_qubits])
        qml.RY(weights[layer, param_index + 2], wires=(i + 1) % n_qubits)

# 构建电路1（不包含测量操作，最后1bit作为切割点）(4bit)
def build_circuits1(inputs, weights, qubits):
    embedding_circuit(inputs, qubits)
    for layer in range(n_layers):
        conv_circuit_1(layer, qubits, weights)

# 构建电路2（不包含测量操作，最后1bit作为切割点）(5bit)
def build_circuits2(inputs, weights, qubits):
    embedding_circuit(inputs, qubits)
    for layer in range(n_layers):
        conv_circuit_2(layer, qubits, weights)

############################################################################
# 测量
@qml.qnode(dev_1, interface="torch")
def circuit_front_1(inputs, weights):
    build_circuits1(inputs, weights, n_qubits_1)
    # return qml.probs(wires=[i for i in range(n_qubits_1)]) # return numpy.array()
    return qml.probs(wires=n_qubits_1 - 1)


@qml.qnode(dev_1, interface="torch")
def circuit_front_2(inputs, weights):
    build_circuits1(inputs, weights, n_qubits_1)
    qml.Hadamard(wires=n_qubits_1 - 1)
    # return qml.probs(wires=[i for i in range(n_qubits_1)])
    return qml.probs(wires=n_qubits_1 - 1)


@qml.qnode(dev_1, interface="torch")
def circuit_front_3(inputs, weights):
    build_circuits1(inputs, weights, n_qubits_1)
    qml.RX(np.pi / 2, wires=n_qubits_1 - 1)
    # return qml.probs(wires=[i for i in range(n_qubits_1)])
    return qml.probs(wires=n_qubits_1 - 1)

############################################################################
# 制备
# 定义量子节点
@qml.qnode(dev_2, interface="torch")
def circuit_back_1(inputs, weights):
    build_circuits2(inputs, weights, n_qubits_2)
    return qml.probs(wires=[i for i in range(n_qubits_2)])


@qml.qnode(dev_2, interface="torch")
def circuit_back_2(inputs, weights):
    qml.PauliX(wires=0)
    build_circuits2(inputs, weights, n_qubits_2)
    return qml.probs(wires=[i for i in range(n_qubits_2)])


@qml.qnode(dev_2, interface="torch")
def circuit_back_3(inputs, weights):
    qml.Hadamard(wires=0)
    build_circuits2(inputs, weights, n_qubits_2)
    return qml.probs(wires=[i for i in range(n_qubits_2)])


@qml.qnode(dev_2, interface="torch")
def circuit_back_4(inputs, weights):
    qml.PauliX(wires=0)
    qml.Hadamard(wires=0)
    build_circuits2(inputs, weights, n_qubits_2)
    return qml.probs(wires=[i for i in range(n_qubits_2)])


@qml.qnode(dev_2, interface="torch")
def circuit_back_5(inputs, weights):
    qml.RX(np.pi / 2, wires=0)
    build_circuits2(inputs, weights, n_qubits_2)
    return qml.probs(wires=[i for i in range(n_qubits_2)])


@qml.qnode(dev_2, interface="torch")
def circuit_back_6(inputs, weights):
    qml.RX(-np.pi / 2, wires=0)
    build_circuits2(inputs, weights, n_qubits_2)
    return qml.probs(wires=[i for i in range(n_qubits_2)])


def calculate_probabilities(tensor):
    # 假设tensor的形状是 [batch_size, num_probs]
    # tensor中的偶数索引代表事件0，奇数索引代表事件1
    prob_event_0 = tensor[:, 0::2].sum(dim=1)  # 沿着第二维求和，取间隔为2的元素，开始于索引0
    prob_event_1 = tensor[:, 1::2].sum(dim=1)  # 同上，但开始于索引1

    # 将结果堆叠成一个新的tensor，形状为 [batch_size, 2]
    result_tensor = torch.stack([prob_event_0,prob_event_1], dim=1)
    return result_tensor


def combine_outputs(probabilities_front_1, probabilities_front_2, probabilities_front_3, output_back_1, output_back_2, output_back_3, 
                    output_back_4, output_back_5, output_back_6):
    # 将所有后端输出转为列表
    res2 = torch.stack([output_back_1, output_back_2, output_back_3,
                        output_back_4, output_back_5, output_back_6], dim=1) # torch.Size([2**n_qubits_2]) => torch.Size([batch_size, 6, 2**n_qubits_2])

    result_1 = torch.cat((probabilities_front_1, probabilities_front_2, probabilities_front_3), dim=1) # torch.Size([batch_size, 6])

    # 计算nz, nx, ny
    nz = result_1[:, 0] - result_1[:, 1] # torch.Size([batch_size])
    nx = result_1[:, 2] - result_1[:, 3]
    ny = result_1[:, 4] - result_1[:, 5]

    # 计算c系数
    c0 = (1 + nz) / 2 # torch.Size([batch_size])
    c1 = (1 - nz) / 2
    c2 = nx / 2
    c3 = -c2
    c4 = ny / 2
    c5 = -c4

    c_list = torch.stack([c0, c1, c2, c3, c4, c5], dim=1) # torch.Size([batch_size, 6])

    # 计算最终结果
    # result_list = torch.zeros_like(res2[0])  # 假设所有输出批次大小和维度相同
    result_list = torch.zeros_like(output_back_1) # torch.Size([batch_size, 2**n_qubits_2])

    for j in range(c_list.shape[1]):
        result_list += c_list[:, j].unsqueeze(1) * res2[:, j, :]

    return result_list


class QNN(nn.Module):
    def __init__(self):
        super(QNN, self).__init__()
        # self.fc1 = nn.Linear(3*128*128, 8) 
        # self.n_qubits=5
        # self.output_dim=3
        # self.mlp_output = nn.Linear(2**self.n_qubits,self.output_dim)

        # self.activation_func = nn.ELU()
        self.flatten_layer = nn.Flatten()
        # 定义共享权重张量
        self.shared_weights_1 = nn.Parameter(torch.rand((n_layers, 2 * (n_qubits_1 - 1) * 3)), requires_grad=True)
        weight_shapes_1 = {"weights": (n_layers, 2 * (n_qubits_1 - 1) * 3)} # 指定weights的形状，参数配置？
        self.QLayer_front_1 = qml.qnn.TorchLayer(circuit_front_1, weight_shapes_1)  # QNode ==> TorchLayer
        self.QLayer_front_2 = qml.qnn.TorchLayer(circuit_front_2, weight_shapes_1)  # QNode ==> TorchLayer
        self.QLayer_front_3 = qml.qnn.TorchLayer(circuit_front_3, weight_shapes_1)  # QNode ==> TorchLayer
        self.QLayer_front_1.weights.data = self.shared_weights_1
        self.QLayer_front_2.weights.data = self.shared_weights_1
        self.QLayer_front_3.weights.data = self.shared_weights_1

        self.shared_weights_2 = nn.Parameter(torch.rand((n_layers, (n_qubits_2 - 1) * 3)), requires_grad=True)
        weight_shapes_2 = {"weights": (n_layers, (n_qubits_2 - 1) * 3)} # 指定weights的形状，参数配置？
        self.QLayer_back_1 = qml.qnn.TorchLayer(circuit_back_1, weight_shapes_2)  # QNode ==> TorchLayer
        self.QLayer_back_2 = qml.qnn.TorchLayer(circuit_back_2, weight_shapes_2)  # QNode ==> TorchLayer
        self.QLayer_back_3 = qml.qnn.TorchLayer(circuit_back_3, weight_shapes_2)  # QNode ==> TorchLayer
        self.QLayer_back_4 = qml.qnn.TorchLayer(circuit_back_4, weight_shapes_2)  # QNode ==> TorchLayer
        self.QLayer_back_5 = qml.qnn.TorchLayer(circuit_back_5, weight_shapes_2)  # QNode ==> TorchLayer
        self.QLayer_back_6 = qml.qnn.TorchLayer(circuit_back_6, weight_shapes_2)  # QNode ==> TorchLayer
        self.QLayer_back_1.weights.data = self.shared_weights_2
        self.QLayer_back_2.weights.data = self.shared_weights_2
        self.QLayer_back_3.weights.data = self.shared_weights_2
        self.QLayer_back_4.weights.data = self.shared_weights_2
        self.QLayer_back_5.weights.data = self.shared_weights_2
        self.QLayer_back_6.weights.data = self.shared_weights_2

    def forward(self, inputs): # inputs = stack(batch_size, x], dim=0)


        output_front_1=[]
        output_front_2=[]
        output_front_3=[]

        output_back_1=[]
        output_back_2=[]
        output_back_3=[]
        output_back_4=[]
        output_back_5=[]
        output_back_6=[]
        for i in inputs:
            output_front_1.append(self.QLayer_front_1(i[:n_qubits_1]))
            output_front_2.append(self.QLayer_front_2(i[:n_qubits_1]))
            output_front_3.append(self.QLayer_front_3(i[:n_qubits_1]))
            output_back_1.append(self.QLayer_back_1(i[n_qubits_1-1:]))
            output_back_2.append(self.QLayer_back_2(i[n_qubits_1-1:]))
            output_back_3.append(self.QLayer_back_3(i[n_qubits_1-1:]))
            output_back_4.append(self.QLayer_back_4(i[n_qubits_1-1:]))
            output_back_5.append(self.QLayer_back_5(i[n_qubits_1-1:]))
            output_back_6.append(self.QLayer_back_6(i[n_qubits_1-1:]))
        
        output_front_1 = torch.stack(output_front_1, dim=0)
        output_front_2 = torch.stack(output_front_2, dim=0)
        output_front_3 = torch.stack(output_front_3, dim=0)
        output_back_1 = torch.stack(output_back_1, dim=0)
        output_back_2 = torch.stack(output_back_2, dim=0)
        output_back_3 = torch.stack(output_back_3, dim=0)
        output_back_4 = torch.stack(output_back_4, dim=0)
        output_back_5 = torch.stack(output_back_5, dim=0)
        output_back_6 = torch.stack(output_back_6, dim=0)

        combined_outputs = combine_outputs(output_front_1, output_front_2, output_front_3,
                                            output_back_1, output_back_2, output_back_3,
                                            output_back_4, output_back_5, output_back_6)

        x = self.flatten_layer(combined_outputs)
        print(x)
        # 经过最后的线性层和激活函数
        # output_tensor = self.mlp_output(combined_outputs)

        # return output_tensor
        return combined_outputs